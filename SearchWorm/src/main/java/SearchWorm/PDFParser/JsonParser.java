package SearchWorm.PDFParser;

import org.json.JSONArray;
import org.json.JSONObject;

import java.util.HashMap;
import java.util.Map;

/**
 * Created by abhinav on 7/12/15.
 */
public class JsonParser {
    public static Map<Integer, String> func(String s) {
//        String s = "{\"took\":597,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0},\"hits\":{\"total\":83,\"max_score\":0.58358806,\"hits\":[{\"_shard\":0,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"Hadoop: The Definitive Guide\",\"_id\":\"573\",\"_score\":0.58358806,\"_source\":{\"content\":\"Table 16-3. Mapper output\\nTrackId UserId\\nIntWritable IntWritable\\n222 11115\\n225 11113\\n223 11117\\n225 11115\\nTable 16-4. Reducer output\\nTrackId #listeners\\nIntWritable IntWritable\\n222 1\\n225 2\\n223 1\\nSumming the track totals\\nThe Sum job is relatively simple; it just adds up the values we are interested in for each\\ntrack.\\nThe input data is again the raw text files, but in this case, it is handled quite\\ndifferently. The desired end result is a number of totals (unique listener count, play\\ncount, scrobble count, radio listen count, skip count) associated with each track. To\\nsimplify things, we use an intermediate TrackStats object generated using Hadoop\\nRecord I/O, which implements WritableComparable (so it can be used as output) to hold\\nthese values. The mapper creates a TrackStats object and sets the values on it for each\\nline in the file, except for the unique listener count, which is left empty (it will be filled\\nin by the final merge job):\\npublic void map(LongWritable position, Text rawLine, \\n    OutputCollector<IntWritable, TrackStats> output, Reporter reporter)\\n    throws IOException {\\n    \\n  String[] parts = (rawLine.toString()).split(\\\" \\\");\\n  int trackId = Integer.parseInt(parts[TrackStatisticsProgram.COL_TRACKID]);\\n  int scrobbles = Integer.parseInt(parts[TrackStatisticsProgram.COL_SCROBBLES]);\\n  int radio = Integer.parseInt(parts[TrackStatisticsProgram.COL_RADIO]);\\n  int skip = Integer.parseInt(parts[TrackStatisticsProgram.COL_SKIP]);\\n  // set number of listeners to 0 (this is calculated later) \\n  // and other values as provided in text file\\n  TrackStats trackstat = new TrackStats(0, scrobbles + radio, scrobbles, radio, skip);\\n  output.collect(new IntWritable(trackId), trackstat);\\n}\\nSumMapper.\\n550 | Chapter 16: Case Studies\\nwww.it-ebooks.info\\n\",\"timeStamp\":\"12/07/2015 22:13:46\"},\"_explanation\":{\"value\":0.583588,\"description\":\"weight(_all:count in 8) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.583588,\"description\":\"score(doc=8,freq=6.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.58358806,\"description\":\"fieldWeight in 8, product of:\",\"details\":[{\"value\":2.4494898,\"description\":\"tf(freq=6.0), with freq of:\",\"details\":[{\"value\":6.0,\"description\":\"termFreq=6.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.0625,\"description\":\"fieldNorm(doc=8)\"}]}]}]}},{\"_shard\":4,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"Hadoop: The Definitive Guide\",\"_id\":\"616\",\"_score\":0.58358806,\"_source\":{\"content\":\"If you think of pipe assemblies like a Java class, then a Flow is like a Java Object instance\\n(Figure 16-17). That is, the same pipe assembly can be “instantiated” many times into\\nnew Flows, in the same application, without fear of any interference between them.\\nThis allows pipe assemblies to be created and shared like standard Java libraries.\\nFigure 16-17. A Flow\\nCascading in Practice\\nNow that we know what Cascading is and have a good idea how it works, what does\\nan application written in Cascading look like? See Example 16-2.\\nExample 16-2. Word count and sort\\nScheme sourceScheme =\\n  new TextLine(new Fields(\\\"line\\\")); \\nTap source =\\n  new Hfs(sourceScheme, inputPath); \\nScheme sinkScheme = new TextLine(); \\nTap sink =\\n  new Hfs(sinkScheme, outputPath, SinkMode.REPLACE); \\nPipe assembly = new Pipe(\\\"wordcount\\\"); \\nString regexString = \\\"(?<!\\\\\\\\pL)(?=\\\\\\\\pL)[^ ]*(?<=\\\\\\\\pL)(?!\\\\\\\\pL)\\\";\\nFunction regex = new RegexGenerator(new Fields(\\\"word\\\"), regexString);\\nassembly =\\n  new Each(assembly, new Fields(\\\"line\\\"), regex); \\nassembly =\\n  new GroupBy(assembly, new Fields(\\\"word\\\")); \\nAggregator count = new Count(new Fields(\\\"count\\\"));\\nassembly = new Every(assembly, count); \\nassembly =\\n  new GroupBy(assembly, new Fields(\\\"count\\\"), new Fields(\\\"word\\\")); \\nCascading | 593\\nwww.it-ebooks.info\\n\",\"timeStamp\":\"12/07/2015 22:13:47\"},\"_explanation\":{\"value\":0.583588,\"description\":\"weight(_all:count in 17) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.583588,\"description\":\"score(doc=17,freq=6.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.58358806,\"description\":\"fieldWeight in 17, product of:\",\"details\":[{\"value\":2.4494898,\"description\":\"tf(freq=6.0), with freq of:\",\"details\":[{\"value\":6.0,\"description\":\"termFreq=6.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.0625,\"description\":\"fieldNorm(doc=17)\"}]}]}]}},{\"_shard\":1,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"Hadoop: The Definitive Guide\",\"_id\":\"618\",\"_score\":0.55155367,\"_source\":{\"content\":\"Example 16-3. Creating a SubAssembly\\npublic class ParseWordsAssembly extends SubAssembly \\n  {\\n  public ParseWordsAssembly(Pipe previous)\\n    {\\n    String regexString = \\\"(?<!\\\\\\\\pL)(?=\\\\\\\\pL)[^ ]*(?<=\\\\\\\\pL)(?!\\\\\\\\pL)\\\";\\n    Function regex = new RegexGenerator(new Fields(\\\"word\\\"), regexString);\\n    previous = new Each(previous, new Fields(\\\"line\\\"), regex);\\n    String exprString = \\\"word.toLowerCase()\\\";\\n    Function expression =\\n      new ExpressionFunction(new Fields(\\\"word\\\"), exprString, String.class); \\n    previous = new Each(previous, new Fields(\\\"word\\\"), expression);\\n    setTails(previous); \\n    }\\n  }\\nWe subclass the SubAssembly class, which is itself a kind of Pipe.\\nWe create a Java expression function that will call toLowerCase() on the String value\\nin the field named “word.” We must also pass in the Java type the expression expects\\n“word” to be, in this case, String. (http://www.janino.net/ is used under the covers.)\\nWe must tell the SubAssembly superclass where the tail ends of our pipe subassembly\\nare.\\nFirst, we create a SubAssembly pipe to hold our “parse words” pipe assembly. Since this\\nis a Java class, it can be reused in any other application, as long as there is an incoming\\nfield named “word” (Example 16-4). Note that there are ways to make this function\\neven more generic, but they are covered in the Cascading User Guide.\\nExample 16-4. Extending word count and sort with a SubAssembly\\nScheme sourceScheme = new TextLine(new Fields(\\\"line\\\"));\\nTap source = new Hfs(sourceScheme, inputPath);\\nScheme sinkScheme = new TextLine(new Fields(\\\"word\\\", \\\"count\\\"));\\nTap sink = new Hfs(sinkScheme, outputPath, SinkMode.REPLACE);\\nPipe assembly = new Pipe(\\\"wordcount\\\");\\nassembly =\\n  new ParseWordsAssembly(assembly); \\nassembly = new GroupBy(assembly, new Fields(\\\"word\\\"));\\nAggregator count = new Count(new Fields(\\\"count\\\"));\\nassembly = new Every(assembly, count);\\nassembly = new GroupBy(assembly, new Fields(\\\"count\\\"), new Fields(\\\"word\\\"));\\nCascading | 595\\nwww.it-ebooks.info\\n\",\"timeStamp\":\"12/07/2015 22:13:47\"},\"_explanation\":{\"value\":0.55155367,\"description\":\"weight(_all:count in 17) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.55155367,\"description\":\"score(doc=17,freq=7.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.5515537,\"description\":\"fieldWeight in 17, product of:\",\"details\":[{\"value\":2.6457512,\"description\":\"tf(freq=7.0), with freq of:\",\"details\":[{\"value\":7.0,\"description\":\"termFreq=7.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.0546875,\"description\":\"fieldNorm(doc=17)\"}]}]}]}},{\"_shard\":3,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"CCNA Cisco Certified Network Associate Study Guide\",\"_id\":\"627\",\"_score\":0.51063955,\"_source\":{\"content\":\"592 Appendix B \\u0002 Configuring the Catalyst 1900 Switch\\nConfiguring Port Security\\nPort security is a way of stopping users from plugging a hub into their jack \\nin their office or cubicle and adding a bunch of hosts without your knowl-\\nedge. By default, 132 hardware addresses can be allowed on a single switch \\ninterface. To change this, use the interface command port secure max-\\nmac-count. \\nThe following switch output shows the command port secure max-\\nmac-count being set on interface 0/2 to allow only one entry. \\nTodd1900EN#config t\\nEnter configuration commands, one per line.  End with \\nCNTL/Z\\nTodd1900EN(config)#int e0/2\\nTodd1900EN(config-if)#port secure ?\\n  max-mac-count  Maximum number of addresses allowed on \\nthe port\\n  <cr>\\nTodd1900EN(config-if)#port secure max-mac-count ?\\n  <1-132>  Maximum mac address count for this secure port\\nTodd1900EN(config-if)#port secure max-mac-count 1\\nThe secured port or ports you create can use either static or sticky-learned \\nhardware addresses. If the hardware addresses on a secured port are not stat-\\nically assigned, the port sticky-learns the source address of incoming frames \\nand automatically assigns them as permanent addresses. Sticky-learns is a \\nterm Cisco uses for a port dynamically finding a source hardware address \\nand creating a permanent entry in the MAC filter table. \\nUsing the Show Version Command\\nYou can use the show version command to view basic information about \\nthe switch. This includes how long the switch has been running, the IOS ver-\\nsion, and the base MAC address of the switch. \\nThis MAC address is important because if you lose your password, there \\nis no password recovery on the 1900 switch. You need to send Cisco this \\nMAC address, and they’ll send you a password that will allow you to get into \\nyour switch. \\n\",\"timeStamp\":\"12/07/2015 21:15:25\"},\"_explanation\":{\"value\":0.5106395,\"description\":\"weight(_all:count in 37) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.5106395,\"description\":\"score(doc=37,freq=6.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.51063955,\"description\":\"fieldWeight in 37, product of:\",\"details\":[{\"value\":2.4494898,\"description\":\"tf(freq=6.0), with freq of:\",\"details\":[{\"value\":6.0,\"description\":\"termFreq=6.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.0546875,\"description\":\"fieldNorm(doc=37)\"}]}]}]}},{\"_shard\":0,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"Hadoop: The Definitive Guide\",\"_id\":\"617\",\"_score\":0.505402,\"_source\":{\"content\":\"FlowConnector flowConnector = new FlowConnector();\\nFlow flow =\\n  flowConnector.connect(\\\"word-count\\\", source, sink, assembly); \\nflow.complete();\\nWe create a new Scheme that reads simple text files and emits a new Tuple for each\\nline in a field named “line,” as declared by the Fields instance.\\nWe create a new Scheme that writes simple text files and expects a Tuple with any\\nnumber of fields/values. If more than one value, they will be tab-delimited in the\\noutput file.\\nWe create source and sink Tap instances that reference the input file and output\\ndirectory, respectively. The sink Tap will overwrite any file that may already exist.\\nWe construct the head of our pipe assembly, and name it “wordcount.” This name\\nis used to bind the source and sink taps to the assembly. Multiple heads or tails\\nwould require unique names.\\nWe construct an Each pipe with a function that will parse the “line” field into a new\\nTuple for each word encountered.\\nWe construct a GroupBy pipe that will create a new Tuple grouping for each unique\\nvalue in the field “word.”\\nWe construct an Every pipe with an Aggregator that will count the number of\\nTuples in every unique word group. The result is stored in a field named “count.”\\nWe construct a GroupBy pipe that will create a new Tuple grouping for each unique\\nvalue in the field “count” and secondary sort each value in the field “word.” The\\nresult will be a list of “count” and “word” values with “count” sorted in increasing\\norder.\\nWe connect the pipe assembly to its sources and sinks into a Flow, and then execute\\nthe Flow on the cluster.\\nIn the example, we count the words encountered in the input document, and we sort\\nthe counts in their natural order (ascending). And if some words have the same “count”\\nvalue, these words are sorted in their natural order (alphabetical).\\nOne obvious problem with this example is that some words might have uppercase\\nletters; for example, “the” and “The” when the word comes at the beginning of a sen-\\ntence. So we might decide to insert a new operation to force all the words to\\nlowercase, but we realize that all future applications that need to parse words from\\ndocuments should have the same behavior, so we decide to create a reusable pipe\\nSubAssembly, just like we would by creating a subroutine in a traditional application\\n(see Example 16-3).\\n594 | Chapter 16: Case Studies\\nwww.it-ebooks.info\\n\",\"timeStamp\":\"12/07/2015 22:13:47\"},\"_explanation\":{\"value\":0.505402,\"description\":\"weight(_all:count in 17) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.505402,\"description\":\"score(doc=17,freq=8.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.5054021,\"description\":\"fieldWeight in 17, product of:\",\"details\":[{\"value\":2.828427,\"description\":\"tf(freq=8.0), with freq of:\",\"details\":[{\"value\":8.0,\"description\":\"termFreq=8.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.046875,\"description\":\"fieldNorm(doc=17)\"}]}]}]}},{\"_shard\":2,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"CCNA Cisco Certified Network Associate Study Guide\",\"_id\":\"330\",\"_score\":0.505402,\"_source\":{\"content\":\"Review Questions 291\\nReview Questions\\n1. What is the routing algorithm used by RIP?\\nA. Routed information\\nB. Link together\\nC. Link state\\nD. Distance vector\\n2. What is the routing algorithm used by IGRP?\\nA. Routed information\\nB. Link together\\nC. Link state\\nD. Distance vector\\n3. Which command can you type at the router prompt to verify the \\nbroadcast frequency for IGRP?\\nA. sh ip route\\nB. sh ip protocol\\nC. sh ip broadcast\\nD. debug ip igrp \\n4. What is the routing metric used by RIP?\\nA. Count to infinity\\nB. Hop count\\nC. TTL\\nD. Bandwidth, delay\\n\",\"timeStamp\":\"12/07/2015 21:15:22\"},\"_explanation\":{\"value\":0.505402,\"description\":\"weight(_all:count in 1) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.505402,\"description\":\"score(doc=1,freq=2.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.5054021,\"description\":\"fieldWeight in 1, product of:\",\"details\":[{\"value\":1.4142135,\"description\":\"tf(freq=2.0), with freq of:\",\"details\":[{\"value\":2.0,\"description\":\"termFreq=2.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.09375,\"description\":\"fieldNorm(doc=1)\"}]}]}]}},{\"_shard\":0,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"Hadoop: The Definitive Guide\",\"_id\":\"585\",\"_score\":0.42116836,\"_source\":{\"content\":\"pruning, map-side aggregations, and other features, the compiler tries to create plans\\nthat can optimize the runtime for the query.\\nData pipelines using Hive\\nAdditionally, the ability provided by Hive in terms of expressing data pipelines in SQL\\ncan and has provided the much needed flexibility in putting these pipelines together in\\nan easy and expedient manner. This is especially useful for organizations and products\\nthat are still evolving and growing. Many of the operations needed in processing data\\npipelines are the well-understood SQL operations like join, group by, and distinct ag-\\ngregations. With Hive’s ability to convert SQL into a series of Hadoop MapReduce\\njobs, it becomes fairly easy to create and maintain these pipelines. We illustrate these\\nfacets of Hive in this section by using an example of a hypothetical ad network and\\nshowing how some typical aggregated reports needed by the advertisers can be com-\\nputed using Hive. As an example, assuming that an online ad network stores informa-\\ntion on ads in a table named dim_ads and stores all the impressions served to that ad in\\na table named impression_logs in Hive, with the latter table being partitioned by date,\\nthe daily impression numbers (both unique and total by campaign, that are routinely\\ngiven by ad networks to the advertisers) for 2008-12-01 are expressible as the following\\nSQL in Hive:\\nSELECT a.campaign_id, count(1), count(DISTINCT b.user_id)\\nFROM dim_ads a JOIN impression_logs b ON(b.ad_id = a.ad_id)\\nWHERE b.dateid = '2008-12-01'\\nGROUP BY a.campaign_id;\\nThis would also be the typical SQL statement that one could use in other RDBMSs such\\nas Oracle, DB2, and so on.\\nIn order to compute the daily impression numbers by ad and account from the same\\njoined data as earlier, Hive provides the ability to do multiple group bys simultaneously\\nas shown in the following query (SQL-like but not strictly SQL):\\nFROM(\\n  SELECT a.ad_id, a.campaign_id, a.account_id, b.user_id\\n  FROM dim_ads a JOIN impression_logs b ON (b.ad_id = a.ad_id)\\n  WHERE b.dateid = '2008-12-01') x\\nINSERT OVERWRITE DIRECTORY 'results_gby_adid'\\n  SELECT x.ad_id, count(1), count(DISTINCT x.user_id) GROUP BY x.ad_id\\nINSERT OVERWRITE DIRECTORY 'results_gby_campaignid'\\n  SELECT x.campaign_id, count(1), count(DISTINCT x.user_id) GROUP BY x.campaign_id\\nINSERT OVERWRITE DIRECTORY 'results_gby_accountid'\\n  SELECT x.account_id, count(1), count(DISTINCT x.user_id) GROUP BY x.account_id;\\nIn one of the optimizations that is being added to Hive, the query can be converted into\\na sequence of Hadoop MapReduce jobs that are able to scale with data skew. Essen-\\ntially, the join is converted into one MapReduce job and the three group bys are con-\\nverted into four MapReduce jobs, with the first one generating a partial aggregate on\\nunique_id. This is especially useful because the distribution of impression_logs over\\nunique_id is much more uniform as compared to ad_id (typically in an ad network, a\\n562 | Chapter 16: Case Studies\\nwww.it-ebooks.info\\n\",\"timeStamp\":\"12/07/2015 22:13:46\"},\"_explanation\":{\"value\":0.42116836,\"description\":\"weight(_all:count in 11) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.42116836,\"description\":\"score(doc=11,freq=8.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.4211684,\"description\":\"fieldWeight in 11, product of:\",\"details\":[{\"value\":2.828427,\"description\":\"tf(freq=8.0), with freq of:\",\"details\":[{\"value\":8.0,\"description\":\"termFreq=8.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.0390625,\"description\":\"fieldNorm(doc=11)\"}]}]}]}},{\"_shard\":2,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"CCNA Cisco Certified Network Associate Study Guide\",\"_id\":\"664\",\"_score\":0.42116836,\"_source\":{\"content\":\" Review Questions\\n \\n629\\n \\n22.\\n \\nWhich of the following ports will set a hardware address on port e0/4 \\nto only MAC address 00A0.2448.60A5?\\n \\nA.\\n \\nint e0/4 set MAC  00A0.2448.60A5\\n \\nB.\\n \\nTodd1900EN(config)#\\n \\nmac-address-table restricted \\nstatic 00A0.2448.60A5 e0/2\\n \\n \\n \\nC.\\n \\nTodd1900EN(config)#\\n \\nmac-address-table permanent \\n00A0.2448.60A5 e0/4\\n \\nD.\\n \\nTodd1900EN(config-if)#\\n \\nport secure max-mac-count \\n00A0.2448.60A5\\n \\n23.\\n \\nWhich of the following commands allows only port e0/5 on a 1900 \\nswitch to communicate with hardware address 00A0.246E.0FA8?\\n \\nA.\\n \\nint e0/5 out 00A0.246E.0FA8\\n \\nB.\\n \\nTodd1900EN(config)#\\n \\nmac-address-table permanent \\n00A0.246E.0FA8 e0/4\\n \\nC.\\n \\nTodd1900EN(config-if)#\\n \\nport secure max-mac-count \\n00A0.246E.0FA8\\n \\n \\n \\nD.\\n \\nTodd1900EN(config)#\\n \\nmac-address-table restricted \\nstatic 00A0.246E.0FA8 e0/2 e0/5\\n\",\"timeStamp\":\"12/07/2015 21:15:25\"},\"_explanation\":{\"value\":0.42116836,\"description\":\"weight(_all:count in 43) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.42116836,\"description\":\"score(doc=43,freq=2.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.4211684,\"description\":\"fieldWeight in 43, product of:\",\"details\":[{\"value\":1.4142135,\"description\":\"tf(freq=2.0), with freq of:\",\"details\":[{\"value\":2.0,\"description\":\"termFreq=2.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.078125,\"description\":\"fieldNorm(doc=43)\"}]}]}]}},{\"_shard\":0,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"CCNA Cisco Certified Network Associate Study Guide\",\"_id\":\"299\",\"_score\":0.4169354,\"_source\":{\"content\":\"260 Chapter 5 \\u0002 IP Routing\\nRouting loops can occur because every router is not updated close to the \\nsame time. Let’s say that the interface to Network 5 in Figure 5.8 fails. All \\nrouters know about Network 5 from Router E. Router A, in its tables, has \\na path to Network 5 through Routers B, C, and E. When Network 5 fails, \\nRouter E tells RouterC. This causes Router C to stop routing to Network 5 \\nthrough Router E. But Routers A, B, and D don’t know about Network 5 \\nyet, so they keep sending out update information. Router C will eventually \\nsend out its update and cause B to stop routing to Network 5, but Routers \\nA and D are still not updated. To them, it appears that Network 5 is still \\navailable through Router B with a metric of three. \\nF I G U R E 5 . 8 Routing loop example\\nRouter A sends out its regular 30-second “Hello, I’m still here—these are \\nthe links I know about” message, which includes reachability for Network 5. \\nRouters B and D then receive the wonderful news that Network 5 can be \\nreached from Router A, so they send out the information that Network 5 is \\navailable. Any packet destined for Network 5 will go to Router A, to Router \\nB, and then back to Router A. This is a routing loop—how do you stop it? \\nMaximum Hop Count\\nThe routing loop problem just described is called counting to infinity, and \\nit’s caused by gossip and wrong information being communicated and prop-\\nagated throughout the internetwork. Without some form of intervention, the \\nhop count increases indefinitely each time a packet passes through a router. \\nOne way of solving this problem is to define a maximum hop count. Dis-\\ntance vector (RIP) permits a hop count of up to 15, so anything that requires \\nRouterA\\nRouterD\\nRouterB RouterC RouterE\\nNetwork 3 Network 4 Network 5\\n56K T3\\n\",\"timeStamp\":\"12/07/2015 21:15:22\"},\"_explanation\":{\"value\":0.4169354,\"description\":\"weight(_all:count in 101) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.4169354,\"description\":\"score(doc=101,freq=4.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.41693544,\"description\":\"fieldWeight in 101, product of:\",\"details\":[{\"value\":2.0,\"description\":\"tf(freq=4.0), with freq of:\",\"details\":[{\"value\":4.0,\"description\":\"termFreq=4.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.0546875,\"description\":\"fieldNorm(doc=101)\"}]}]}]}},{\"_shard\":3,\"_node\":\"AY8TTIhUTSSNHqF8Wd6Uhw\",\"_index\":\"searchworm\",\"_type\":\"CCNA Cisco Certified Network Associate Study Guide\",\"_id\":\"336\",\"_score\":0.41265905,\"_source\":{\"content\":\"Answers to Review Questions 297\\nAnswers to Review Questions\\n1. D. RIP uses the distance-vector routing algorithm and uses only hop \\ncount as a metric to determine the pest path to an internetwork.\\n2. D. IGRP is Cisco’s proprietary distance-vector routing algorithm.\\n3. B. The command show ip protocol will show you the configured \\nrouting protocols on your router, which includes the timers. \\n4. B. RIP only uses hop count to determine the best path to a remote \\nnetwork.\\n5. C. The config-router passive-interface command stops updates \\nfrom being sent out an interface.\\n6. D, E. Bandwidth and delay of the line are used by IGRP to determine \\nthe best way to a remote network.\\n7. D. RIP, by default, is only configured to run 15 hops; 16 is deemed \\nunreachable. \\n8. B. Holddowns prevent regular update messages from reinstating a \\ndowned route.\\n9. A. Split horizon will not advertise a route back to the same router it \\nlearned the route from.\\n10. D. Poison reverse is used to communicate to a router that the router \\nunderstands the link is down and that the hop count to that network \\nis set to infinity, or unreachable.\\n11. B. IGRP default administrative distance is 100; RIP’s default admin-\\nistrative distance is 120.\\n12. B. An IP route with a wildcard of all zeroes for the destination net-\\nwork and subnet mask is used to create a default route.\\n\",\"timeStamp\":\"12/07/2015 21:15:22\"},\"_explanation\":{\"value\":0.41265902,\"description\":\"weight(_all:count in 3) [PerFieldSimilarity], result of:\",\"details\":[{\"value\":0.41265902,\"description\":\"score(doc=3,freq=3.0), product of:\",\"details\":[{\"value\":0.99999994,\"description\":\"queryWeight, product of:\",\"details\":[{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.26233077,\"description\":\"queryNorm\"}]},{\"value\":0.41265905,\"description\":\"fieldWeight in 3, product of:\",\"details\":[{\"value\":1.7320508,\"description\":\"tf(freq=3.0), with freq of:\",\"details\":[{\"value\":3.0,\"description\":\"termFreq=3.0\"}]},{\"value\":3.8119812,\"description\":\"idf(docFreq=83, maxDocs=1398)\"},{\"value\":0.0625,\"description\":\"fieldNorm(doc=3)\"}]}]}]}}]}}";
        HashMap<Integer, String> map = new HashMap<Integer, String>();
        JSONObject jObj1 = new JSONObject(s);
        System.out.println(jObj1);

        JSONObject jObj2 = jObj1.getJSONObject("hits");
        System.out.println(jObj2);

        JSONArray jsonArray = jObj2.getJSONArray("hits");
        System.out.println(jsonArray);
        String bookname;
        int pageNumber;
        String content;
        for (int i = 0; i < jsonArray.length(); i++) {
            System.out.println("!!!!!!!!!!BEGIN!!!!!!!!!!!!");
            JSONObject iObj = jsonArray.getJSONObject(i);
            bookname = iObj.getString("_type");
            pageNumber = iObj.getInt("_id");

            content = iObj.getJSONObject("_source").getString("content");
            System.out.println(bookname + " : " + pageNumber + " : " + content);
            map.put(pageNumber, content);
            System.out.println("!!!!!!!!!!!END!!!!!!!!!!!");

        }
        return map;
    }
}